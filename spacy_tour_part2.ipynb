{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A tour of awesome features ofÂ spaCy (part 2/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_vectors_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>mlai</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Road Ahead: Artificial Intelligence will a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can Robots Teach Us How to Be Human?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vehicle DC Voltage Converter Step down 24V to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Which Is The Best Tablet For Kids?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Top 10 Strongest Dog Breeds in the World</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  mlai\n",
       "0  The Road Ahead: Artificial Intelligence will a...     1\n",
       "1               Can Robots Teach Us How to Be Human?     1\n",
       "2  Vehicle DC Voltage Converter Step down 24V to ...     0\n",
       "3                 Which Is The Best Tablet For Kids?     0\n",
       "4           Top 10 Strongest Dog Breeds in the World     0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need to run this, texts.jsonl is already available \n",
    "import jsonlines \n",
    "with jsonlines.open('texts.jsonl', mode='w') as writer:\n",
    "    for text in train_df.title:\n",
    "        writer.write({'text':text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125517\n"
     ]
    }
   ],
   "source": [
    "# word count\n",
    "count = 0\n",
    "for title in train_df.title:\n",
    "    count += len(title.split())\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: spacy pretrain [-h] [-cw 96] [-cd 4] [-er 2000] [-L cosine] [-uv]\n",
      "                      [-d 0.2] [-i 1000] [-bs 3000] [-xw 500] [-nw 5] [-s 0]\n",
      "                      texts_loc vectors_model output_dir\n",
      "\n",
      "    Pre-train the 'token-to-vector' (tok2vec) layer of pipeline components,\n",
      "    using an approximate language-modelling objective. Specifically, we load\n",
      "    pre-trained vectors, and train a component like a CNN, BiLSTM, etc to predict\n",
      "    vectors which match the pre-trained ones. The weights are saved to a directory\n",
      "    after each epoch. You can then pass a path to one of these pre-trained weights\n",
      "    files to the 'spacy train' command.\n",
      "\n",
      "    This technique may be especially helpful if you have little labelled data.\n",
      "    However, it's still quite experimental, so your mileage may vary.\n",
      "\n",
      "    To load the weights back in during 'spacy train', you need to ensure\n",
      "    all settings are the same between pretraining and training. The API and\n",
      "    errors around this need some improvement.\n",
      "    \n",
      "\n",
      "positional arguments:\n",
      "  texts_loc             Path to jsonl file with texts to learn from\n",
      "  vectors_model         Name or path to vectors model to learn from\n",
      "  output_dir            Directory to write models each epoch\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -cw 96, --width 96    Width of CNN layers\n",
      "  -cd 4, --depth 4      Depth of CNN layers\n",
      "  -er 2000, --embed-rows 2000\n",
      "                        Embedding rows\n",
      "  -L cosine, --loss-func cosine\n",
      "                        Loss to use for the objective. L2 or cosine\n",
      "  -uv, --use-vectors    Whether to use the static vectors as input features\n",
      "  -d 0.2, --dropout 0.2\n",
      "                        Dropout\n",
      "  -i 1000, --nr-iter 1000\n",
      "                        Number of iterations to pretrain\n",
      "  -bs 3000, --batch-size 3000\n",
      "                        Number of words per training batch\n",
      "  -xw 500, --max-length 500\n",
      "                        Max words per example.\n",
      "  -nw 5, --min-length 5\n",
      "                        Min words per example.\n",
      "  -s 0, --seed 0        Seed for random number generators\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy pretrain -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy pretrain texts.jsonl en_vectors_web_lg ./pretrained-model\n",
    "#!python -m spacy pretrain texts.jsonl en_vectors_web_lg ./pretrained-model-vecs --use-vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess trian data\n",
    "train_texts = train_df['title']\n",
    "train_cats = [{\"POSITIVE\": bool(y), \"NEGATIVE\": not bool(y)} \n",
    "                 for y in train_df['mlai']]\n",
    "\n",
    "# load and preprocess dev data\n",
    "dev_df = pd.read_csv('dev.csv')\n",
    "dev_texts = dev_df['title']\n",
    "dev_cats = [{\"POSITIVE\": bool(y), \"NEGATIVE\": not bool(y)} \n",
    "                 for y in dev_df['mlai']]\n",
    "\n",
    "train_data = list(zip(train_texts, [{\"cats\": cats} \n",
    "                                     for cats in train_cats]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The Android game for the Matching Clothes App',\n",
       " {'cats': {'POSITIVE': False, 'NEGATIVE': True}})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15896\n",
      "7937\n"
     ]
    }
   ],
   "source": [
    "print(len(train_texts))\n",
    "print(len(dev_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code is modified from spaCy's user guide for TextCategorizer training \n",
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "def main(model=None, n_iter=5, init_tok2vec=None):\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "    # add the text classifier to the pipeline if it doesn't exist\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if \"textcat\" not in nlp.pipe_names:\n",
    "        textcat = nlp.create_pipe(\n",
    "            \"textcat\",\n",
    "            config={\n",
    "                \"exclusive_classes\": True,\n",
    "                \"architecture\": \"simple_cnn\",\n",
    "            }\n",
    "        )\n",
    "        nlp.add_pipe(textcat, last=True)\n",
    "    # otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        textcat = nlp.get_pipe(\"textcat\")\n",
    "\n",
    "    # add label to text classifier\n",
    "    textcat.add_label(\"POSITIVE\")\n",
    "    textcat.add_label(\"NEGATIVE\")\n",
    "\n",
    "    # load the datasets\n",
    "    print(\"Loading data...\")\n",
    "    train_df = pd.read_csv('train.csv')\n",
    "    train_texts = train_df['title']\n",
    "    train_cats = [{\"POSITIVE\": bool(y), \"NEGATIVE\": not bool(y)} \n",
    "                     for y in train_df['mlai']]\n",
    "\n",
    "    dev_df = pd.read_csv('dev.csv')\n",
    "    dev_texts = dev_df['title']\n",
    "    dev_cats = [{\"POSITIVE\": bool(y), \"NEGATIVE\": not bool(y)} \n",
    "                     for y in dev_df['mlai']]\n",
    "\n",
    "    print(\n",
    "        \"Using {} examples ({} training, {} evaluation)\".format(\n",
    "            len(train_texts) + len(dev_texts), len(train_texts), len(dev_texts)\n",
    "        )\n",
    "    )\n",
    "    train_data = list(zip(train_texts, [{\"cats\": cats} for cats in train_cats]))\n",
    "        \n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"textcat\"]\n",
    "    results_df = pd.DataFrame(np.nan, index=range(n_iter), columns=['loss', 'accuracy', 'precision', 'recall', 'f1'])\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
    "        optimizer = nlp.begin_training()\n",
    "        if init_tok2vec is not None:\n",
    "            init_tok2vec = Path(init_tok2vec)\n",
    "            with init_tok2vec.open(\"rb\") as file_:\n",
    "                textcat.model.tok2vec.from_bytes(file_.read())\n",
    "        print(\"Training the model...\")\n",
    "        print(\"{:^5}\\t{:^5}\\t{:^5}\\t{:^5}\\t{:^5}\".format(\"LOSS\", \"A\", \"P\", \"R\", \"F\"))\n",
    "        batch_sizes = compounding(4.0, 32.0, 1.001)\n",
    "        for i in range(n_iter):\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            #random.shuffle(train_data)\n",
    "            batches = minibatch(train_data, size=batch_sizes)\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.2, losses=losses)\n",
    "            with textcat.model.use_params(optimizer.averages):\n",
    "                # evaluate on the dev data split off in load_data()\n",
    "                scores = evaluate(nlp.tokenizer, textcat, dev_texts, dev_cats)\n",
    "            results_df.loc[i,'loss'] = losses[\"textcat\"]\n",
    "            results_df.loc[i,'accuracy'] = scores[\"textcat_a\"]\n",
    "            results_df.loc[i,'precision'] = scores[\"textcat_p\"]\n",
    "            results_df.loc[i,'recall'] = scores[\"textcat_r\"]\n",
    "            results_df.loc[i,'f1'] = scores[\"textcat_f\"]\n",
    "            print(\n",
    "                \"{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}\\t{4:.3f}\".format(  # print a simple table\n",
    "                    losses[\"textcat\"],\n",
    "                    scores[\"textcat_a\"],\n",
    "                    scores[\"textcat_p\"],\n",
    "                    scores[\"textcat_r\"],\n",
    "                    scores[\"textcat_f\"],\n",
    "                )\n",
    "            )\n",
    "    return results_df        \n",
    "                \n",
    "\n",
    "def evaluate(tokenizer, textcat, texts, cats):\n",
    "    docs = (tokenizer(text) for text in texts)\n",
    "    tp = 0.0  # True positives\n",
    "    fp = 1e-8  # False positives\n",
    "    fn = 1e-8  # False negatives\n",
    "    tn = 0.0  # True negatives\n",
    "    for i, doc in enumerate(textcat.pipe(docs)):\n",
    "        gold = cats[i]\n",
    "        for label, score in doc.cats.items():\n",
    "            if label not in gold:\n",
    "                continue\n",
    "            if label == \"NEGATIVE\":\n",
    "                continue\n",
    "            if score >= 0.5 and gold[label] >= 0.5:\n",
    "                tp += 1.0\n",
    "            elif score >= 0.5 and gold[label] < 0.5:\n",
    "                fp += 1.0\n",
    "            elif score < 0.5 and gold[label] < 0.5:\n",
    "                tn += 1\n",
    "            elif score < 0.5 and gold[label] >= 0.5:\n",
    "                fn += 1\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    accuracy = (tp + tn) / (tp + tn + fn + fp)\n",
    "    if (precision + recall) == 0:\n",
    "        f_score = 0.0\n",
    "    else:\n",
    "        f_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return {\"textcat_a\": accuracy, \"textcat_p\": precision, \"textcat_r\": recall, \"textcat_f\": f_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 'en_core_web_lg'\n",
      "Loading data...\n",
      "Using 23833 examples (15896 training, 7937 evaluation)\n",
      "Training the model...\n",
      "LOSS \t  A  \t  P  \t  R  \t  F  \n",
      "6.520\t0.883\t0.874\t0.910\t0.891\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f80fc7daa79a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfix_random_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en_core_web_lg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_tok2vec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pretrained-model-lg-tags/model999.bin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-1c8b15e2d730>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(model, n_iter, init_tok2vec)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                 \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtextcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0;31m# evaluate on the dev data split off in load_data()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/nlp/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, docs, golds, drop, sgd, losses, component_cfg)\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgolds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m                 \u001b[0msgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrehearse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32moptimizers.pyx\u001b[0m in \u001b[0;36mthinc.neural.optimizers.Optimizer.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mops.pyx\u001b[0m in \u001b[0;36mthinc.neural.ops.Ops.clip_gradient\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/nlp/lib/python3.6/site-packages/thinc/neural/util.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(_)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mcupy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mget_array_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "spacy.util.fix_random_seed()\n",
    "main(model='en_core_web_lg', init_tok2vec='pretrained-model-lg-tags/model999.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.util.fix_random_seed()\n",
    "main(model='en_core_web_lg', init_tok2vec='pretrained-model-lg-tags/model999.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 'en_core_web_lg'\n",
      "Loading data...\n",
      "Using 23833 examples (15896 training, 7937 evaluation)\n",
      "Training the model...\n",
      "LOSS \t  A  \t  P  \t  R  \t  F  \n",
      "7.667\t0.878\t0.871\t0.902\t0.886\n",
      "0.073\t0.878\t0.866\t0.908\t0.886\n",
      "0.028\t0.872\t0.862\t0.900\t0.881\n",
      "0.024\t0.872\t0.862\t0.902\t0.881\n",
      "0.018\t0.868\t0.856\t0.900\t0.877\n"
     ]
    }
   ],
   "source": [
    "main(model='en_core_web_lg', init_tok2vec=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n",
      "Loading data...\n",
      "Using 23833 examples (15896 training, 7937 evaluation)\n",
      "Training the model...\n",
      "LOSS \t  A  \t  P  \t  R  \t  F  \n",
      "7.026\t0.871\t0.851\t0.913\t0.881\n",
      "0.065\t0.868\t0.858\t0.897\t0.877\n",
      "0.024\t0.867\t0.859\t0.895\t0.877\n",
      "0.017\t0.859\t0.853\t0.885\t0.869\n",
      "0.016\t0.862\t0.855\t0.888\t0.871\n"
     ]
    }
   ],
   "source": [
    "main(model=None, init_tok2vec='pretrained-model-tags/model999.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n",
      "Loading data...\n",
      "Using 23833 examples (15896 training, 7937 evaluation)\n",
      "Training the model...\n",
      "LOSS \t  A  \t  P  \t  R  \t  F  \n",
      "9.946\t0.850\t0.837\t0.886\t0.861\n",
      "0.081\t0.855\t0.844\t0.888\t0.865\n",
      "0.034\t0.852\t0.842\t0.885\t0.863\n",
      "0.025\t0.851\t0.842\t0.883\t0.862\n",
      "0.021\t0.846\t0.838\t0.877\t0.857\n"
     ]
    }
   ],
   "source": [
    "main(model=None, init_tok2vec=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 'en_core_web_lg'\n",
      "Loading data...\n",
      "Using 23833 examples (15896 training, 7937 evaluation)\n",
      "Training the model...\n",
      "LOSS \t  A  \t  P  \t  R  \t  F  \n",
      "6.693\t0.883\t0.879\t0.902\t0.891\n",
      "0.066\t0.877\t0.869\t0.902\t0.885\n",
      "0.025\t0.874\t0.867\t0.899\t0.883\n",
      "0.021\t0.875\t0.868\t0.899\t0.883\n",
      "0.017\t0.869\t0.863\t0.893\t0.878\n"
     ]
    }
   ],
   "source": [
    "main(model='en_core_web_lg', init_tok2vec='pretrained-model-lg-tags/model50.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n",
      "Loading data...\n",
      "Using 23833 examples (15896 training, 7937 evaluation)\n",
      "Training the model...\n",
      "LOSS \t  A  \t  P  \t  R  \t  F  \n",
      "8.077\t0.856\t0.842\t0.894\t0.868\n",
      "0.078\t0.860\t0.852\t0.888\t0.870\n",
      "0.030\t0.857\t0.852\t0.881\t0.867\n",
      "0.024\t0.855\t0.849\t0.880\t0.864\n",
      "0.018\t0.853\t0.849\t0.876\t0.862\n"
     ]
    }
   ],
   "source": [
    "main(model=None, init_tok2vec='pretrained-model-tags/model50.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
